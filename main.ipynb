{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "FILE_PATH = \"Download_data_RR.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:\n",
    "Download the RNA-RNA interaction database from the link: http://www.rnainter.org/raidMedia/download/Download_data_RR.tar.gz and extract the file in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "Filter the database with Species: Homo sapiens, Category1: miRNA, Category2: mRNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       RNAInterID Interactor1.Symbol Category1      Species1  \\\n",
      "10231  RR00295012      hsa-let-7a-5p     miRNA  Homo sapiens   \n",
      "10232  RR00297665      hsa-let-7a-5p     miRNA  Homo sapiens   \n",
      "10233  RR00478746      hsa-let-7a-5p     miRNA  Homo sapiens   \n",
      "10234  RR00509712      hsa-let-7c-5p     miRNA  Homo sapiens   \n",
      "10235  RR00518781     hsa-miR-101-3p     miRNA  Homo sapiens   \n",
      "\n",
      "      Interactor2.Symbol Category2      Species2               Raw_ID1  \\\n",
      "10231              CCND2      mRNA  Homo sapiens  miRBase:MIMAT0000062   \n",
      "10232               CCR7      mRNA  Homo sapiens  miRBase:MIMAT0000062   \n",
      "10233               E2F2      mRNA  Homo sapiens  miRBase:MIMAT0000062   \n",
      "10234              ERCC6      mRNA  Homo sapiens  miRBase:MIMAT0000064   \n",
      "10235               EZH2      mRNA  Homo sapiens  miRBase:MIMAT0000099   \n",
      "\n",
      "         Raw_ID2   score                                             strong  \\\n",
      "10231   NCBI:894  0.6905  Western blot//RT-PCR//Reporter assay//qRT-PCR/...   \n",
      "10232  NCBI:1236   0.607  Northern blot//Western blot//RT-PCR//Reporter ...   \n",
      "10233  NCBI:1870  0.3664  MTT assay//Western blot//Dual luciferase repor...   \n",
      "10234  NCBI:2074  0.5532  MTT assay//Western blot//Dual luciferase repor...   \n",
      "10235  NCBI:2146   0.882  Northern blot//qPCR//Western blot//RT-PCR//RIP...   \n",
      "\n",
      "                                                    weak  \\\n",
      "10231          HITS-CLIP//Microarray//CLIP-seq//PAR-CLIP   \n",
      "10232                      HITS-CLIP//CLIP-seq//PAR-CLIP   \n",
      "10233                                           CLIP-seq   \n",
      "10234                                           CLIP-seq   \n",
      "10235  HITS-CLIP//miRNA array//CLIP-seq//CLASH//PAR-C...   \n",
      "\n",
      "                                                 predict  \n",
      "10231  Cupid//MirTarget//Bayesian target prediction a...  \n",
      "10232                          Cupid//miRanda//MirTarget  \n",
      "10233                                          MirTarget  \n",
      "10234  Cupid//miRanda//MirTarget//Bayesian target pre...  \n",
      "10235  Bayesian target prediction algorithm//TargetSc...  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(FILE_PATH, sep=\"\\t\", dtype=str)\n",
    "\n",
    "filtered_data = data[(data['Species1'] == 'Homo sapiens') & \n",
    "                     (data['Species2'] == 'Homo sapiens') &\n",
    "                     (data['Category1'] == 'miRNA') & \n",
    "                     (data['Category2'] == 'mRNA')]\n",
    "\n",
    "print(filtered_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "Select any 50000 rows from the filtered dataset and make the columns in the order - RNAInter ID,Interactor1,ID1,Category1,Species1,Interactor2,ID2,Category2,Species2,Score,etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 entries of the miRNA-mRNA interaction database are as follows ...\n",
      "{'RNAInter ID': 'RR03762943', 'Interactor1': 'hsa-miR-1238-5p', 'ID1': 'miRBase:MIMAT0022947', 'Category1': 'miRNA', 'Species1': 'Homo sapiens', 'Interactor2': 'NSD1', 'ID2': 'NCBI:64324', 'Category2': 'mRNA', 'Species2': 'Homo sapiens', 'Score': '0.2537'}\n",
      "{'RNAInter ID': 'RR01692719', 'Interactor1': 'hsa-miR-155-5p', 'ID1': 'miRBase:MIMAT0000646', 'Category1': 'miRNA', 'Species1': 'Homo sapiens', 'Interactor2': 'ADAMTS16', 'ID2': 'NCBI:170690', 'Category2': 'mRNA', 'Species2': 'Homo sapiens', 'Score': '0.2129'}\n",
      "{'RNAInter ID': 'RR03359131', 'Interactor1': 'hsa-miR-371a-5p', 'ID1': 'miRBase:MIMAT0004687', 'Category1': 'miRNA', 'Species1': 'Homo sapiens', 'Interactor2': 'LCOR', 'ID2': 'NCBI:84458', 'Category2': 'mRNA', 'Species2': 'Homo sapiens', 'Score': '0.2986'}\n",
      "{'RNAInter ID': 'RR00791142', 'Interactor1': 'hsa-miR-31-5p', 'ID1': 'miRBase:MIMAT0000089', 'Category1': 'miRNA', 'Species1': 'Homo sapiens', 'Interactor2': 'KCNMA1', 'ID2': 'NCBI:3778', 'Category2': 'mRNA', 'Species2': 'Homo sapiens', 'Score': '0.3989'}\n",
      "{'RNAInter ID': 'RR04939262', 'Interactor1': 'hsa-miR-181a-5p', 'ID1': 'miRBase:MIMAT0000256', 'Category1': 'miRNA', 'Species1': 'Homo sapiens', 'Interactor2': 'TSPAN8', 'ID2': 'NCBI:7103', 'Category2': 'mRNA', 'Species2': 'Homo sapiens', 'Score': '0.2129'}\n",
      "{'RNAInter ID': 'RR06220604', 'Interactor1': 'hsa-miR-3182', 'ID1': 'miRBase:MIMAT0015062', 'Category1': 'miRNA', 'Species1': 'Homo sapiens', 'Interactor2': 'C1orf131', 'ID2': 'NCBI:128061', 'Category2': 'mRNA', 'Species2': 'Homo sapiens', 'Score': '0.201'}\n",
      "{'RNAInter ID': 'RR04815076', 'Interactor1': 'hsa-miR-19a-3p', 'ID1': 'miRBase:MIMAT0000073', 'Category1': 'miRNA', 'Species1': 'Homo sapiens', 'Interactor2': 'TBCA', 'ID2': 'NCBI:6902', 'Category2': 'mRNA', 'Species2': 'Homo sapiens', 'Score': '0.2129'}\n",
      "{'RNAInter ID': 'RR02735215', 'Interactor1': 'hsa-miR-106a-5p', 'ID1': 'miRBase:MIMAT0000103', 'Category1': 'miRNA', 'Species1': 'Homo sapiens', 'Interactor2': 'FAM151B', 'ID2': 'NCBI:167555', 'Category2': 'mRNA', 'Species2': 'Homo sapiens', 'Score': '0.2129'}\n",
      "{'RNAInter ID': 'RR06469837', 'Interactor1': 'hsa-miR-767-5p', 'ID1': 'miRBase:MIMAT0003882', 'Category1': 'miRNA', 'Species1': 'Homo sapiens', 'Interactor2': 'DOT1L', 'ID2': 'NCBI:84444', 'Category2': 'mRNA', 'Species2': 'Homo sapiens', 'Score': '0.1619'}\n",
      "{'RNAInter ID': 'RR02943863', 'Interactor1': 'hsa-miR-422a', 'ID1': 'miRBase:MIMAT0001339', 'Category1': 'miRNA', 'Species1': 'Homo sapiens', 'Interactor2': 'GPRC5B', 'ID2': 'NCBI:51704', 'Category2': 'mRNA', 'Species2': 'Homo sapiens', 'Score': '0.3192'}\n"
     ]
    }
   ],
   "source": [
    "selected_data = filtered_data.sample(n=50000, random_state=42)\n",
    "\n",
    "entries = []\n",
    "for _, row in selected_data.iterrows():\n",
    "    entry = {\n",
    "        'RNAInter ID': row['RNAInterID'],\n",
    "        'Interactor1': row['Interactor1.Symbol'],\n",
    "        'ID1': row['Raw_ID1'],\n",
    "        'Category1': row['Category1'],\n",
    "        'Species1': row['Species1'],\n",
    "        'Interactor2': row['Interactor2.Symbol'],\n",
    "        'ID2': row['Raw_ID2'],\n",
    "        'Category2': row['Category2'],\n",
    "        'Species2': row['Species2'],\n",
    "        'Score': row['score']\n",
    "    }\n",
    "    entries.append(entry)\n",
    "\n",
    "print('First 10 entries of the miRNA-mRNA interaction database are as follows ...')\n",
    "for entry in entries[:10]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4:\n",
    "Create a graph out of the dataset with edges as the interacting miRNA and mRNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency list of a random node: NCBI:5310\n",
      "Neighbor: miRBase:MIMAT0019815, Weight: 1\n",
      "Neighbor: miRBase:MIMAT0004672, Weight: 1\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "for entry in entries:\n",
    "    G.add_edge(entry['ID1'], entry['ID2'], weight=1)\n",
    "\n",
    "# Select a random node\n",
    "random_node = random.choice(list(G.nodes))\n",
    "\n",
    "# Print the adjacency list of the random node\n",
    "print('Adjacency list of a random node:', random_node)\n",
    "for neighbor in G.adj[random_node]:\n",
    "    print(f'Neighbor: {neighbor}, Weight: {G[random_node][neighbor][\"weight\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5:\n",
    "Find the largest connected component from the resulting graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the largest connected component: 16931\n",
      "Adjacency list of a random node in the largest connected component: miRBase:MIMAT0010133\n",
      "Neighbor: NCBI:101060445, Weight: 1\n",
      "Neighbor: NCBI:9337, Weight: 1\n",
      "Neighbor: NCBI:282996, Weight: 1\n",
      "Neighbor: NCBI:219988, Weight: 1\n",
      "Neighbor: NCBI:26084, Weight: 1\n",
      "Neighbor: NCBI:23196, Weight: 1\n",
      "Neighbor: NCBI:65108, Weight: 1\n",
      "Neighbor: NCBI:4800, Weight: 1\n",
      "Neighbor: NCBI:6045, Weight: 1\n",
      "Neighbor: NCBI:166614, Weight: 1\n",
      "Neighbor: NCBI:116496, Weight: 1\n",
      "Neighbor: NCBI:81608, Weight: 1\n",
      "Neighbor: NCBI:57649, Weight: 1\n",
      "Neighbor: NCBI:4249, Weight: 1\n",
      "Neighbor: NCBI:2272, Weight: 1\n",
      "Neighbor: NCBI:11135, Weight: 1\n"
     ]
    }
   ],
   "source": [
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "subgraph = G.subgraph(largest_cc)\n",
    "\n",
    "print('Number of nodes in the largest connected component:', subgraph.number_of_nodes())\n",
    "\n",
    "# Select a random node in the largest connected component\n",
    "random_node = random.choice(list(subgraph.nodes))\n",
    "\n",
    "# Print the adjacency list of the random node in the largest connected component\n",
    "print('Adjacency list of a random node in the largest connected component:', random_node)\n",
    "for neighbor in subgraph.adj[random_node]:\n",
    "    print(f'Neighbor: {neighbor}, Weight: {subgraph[random_node][neighbor][\"weight\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6:\n",
    "Divide the edges into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 train features and labels ...\n",
      "('miRBase:MIMAT0004564', 'NCBI:26267') 1\n",
      "('miRBase:MIMAT0004762', 'NCBI:389874') 1\n",
      "('miRBase:MIMAT0019715', 'NCBI:3746') 1\n",
      "('miRBase:MIMAT0000449', 'NCBI:140460') 1\n",
      "('NCBI:481', 'miRBase:MIMAT0014994') 1\n",
      "('NCBI:64397', 'miRBase:MIMAT0002170') 1\n",
      "('NCBI:6526', 'miRBase:MIMAT0018935') 1\n",
      "('NCBI:89795', 'miRBase:MIMAT0018182') 1\n",
      "('miRBase:MIMAT0018957', 'NCBI:5789') 1\n",
      "('NCBI:7538', 'miRBase:MIMAT0002806') 1\n",
      "First 10 test features and labels ...\n",
      "('miRBase:MIMAT0003225', 'NCBI:148789') 1\n",
      "('miRBase:MIMAT0003165', 'NCBI:26145') 1\n",
      "('miRBase:MIMAT0026640', 'NCBI:7849') 1\n",
      "('miRBase:MIMAT0015086', 'NCBI:6764') 1\n",
      "('NCBI:1317', 'miRBase:MIMAT0000760') 1\n",
      "('miRBase:MIMAT0028116', 'NCBI:673') 1\n",
      "('miRBase:MIMAT0002876', 'NCBI:7021') 1\n",
      "('NCBI:151647', 'miRBase:MIMAT0019709') 1\n",
      "('miRBase:MIMAT0004957', 'NCBI:6722') 1\n",
      "('miRBase:MIMAT0004766', 'NCBI:23554') 1\n"
     ]
    }
   ],
   "source": [
    "# Extract edges from the graph\n",
    "edges = list(G.edges(data=True))\n",
    "\n",
    "# Extract features and labels from the edges\n",
    "edge_features = [(edge[0], edge[1]) for edge in edges]\n",
    "labels = [edge[2]['weight'] for edge in edges]\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(edge_features, labels, test_size=0.2, random_state=42)\\\n",
    "\n",
    "print('First 10 train features and labels ...')\n",
    "for idx in range(10):\n",
    "    print(train_features[idx], train_labels[idx])\n",
    "\n",
    "print('First 10 test features and labels ...')\n",
    "for idx in range(10):\n",
    "    print(test_features[idx], test_labels[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7:\n",
    "Convert the edge list to a DGLGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create a dictionary mapping node IDs to their corresponding indices\n",
    "node_to_index = {node: index for index, node in enumerate(G.nodes)}\n",
    "\n",
    "# Extract source and destination indices from the edges\n",
    "src = [node_to_index[edge[0]] for edge in edges]\n",
    "dst = [node_to_index[edge[1]] for edge in edges]\n",
    "\n",
    "G_dgl = dgl.graph((src, dst))\n",
    "\n",
    "# Add self loops to the graph\n",
    "G_dgl = dgl.add_self_loop(G_dgl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8:\n",
    "Define a simple GNN model for edge prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, num_classes):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_feats)\n",
    "        self.conv2 = GraphConv(hidden_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.conv1(g, features)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9:\n",
    "Define the model and optimizer and prepare the training features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(in_feats=1, hidden_feats=64, num_classes=1).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Prepare features for the nodes (currently only using node IDs)\n",
    "num_nodes = G_dgl.number_of_nodes()\n",
    "features = torch.arange(num_nodes).view(-1, 1).float()\n",
    "\n",
    "# Convert train_labels to PyTorch tensor\n",
    "train_labels_tensor = torch.tensor(train_labels).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10:\n",
    "Train any edge prediction algorithm (a basic GNN with randomly initialized nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 53032.921875\n",
      "Epoch 10, Loss: 15047.396484375\n",
      "Epoch 20, Loss: 1799.99267578125\n",
      "Epoch 30, Loss: 0.773609459400177\n",
      "Epoch 40, Loss: 734.9237670898438\n",
      "Epoch 50, Loss: 527.658203125\n",
      "Epoch 60, Loss: 10.201738357543945\n",
      "Epoch 70, Loss: 61.73427963256836\n",
      "Epoch 80, Loss: 21.539552688598633\n",
      "Epoch 90, Loss: 1.5870113372802734\n",
      "Epoch 100, Loss: 0.4328601062297821\n",
      "Epoch 110, Loss: 0.4879063367843628\n",
      "Epoch 120, Loss: 0.4191320836544037\n",
      "Epoch 130, Loss: 0.4044059216976166\n",
      "Epoch 140, Loss: 0.4211866557598114\n",
      "Epoch 150, Loss: 0.4172968566417694\n",
      "Epoch 160, Loss: 0.3978002965450287\n",
      "Epoch 170, Loss: 0.39099082350730896\n",
      "Epoch 180, Loss: 0.3887788951396942\n",
      "Epoch 190, Loss: 0.38479912281036377\n",
      "Epoch 200, Loss: 0.3818698227405548\n",
      "Epoch 210, Loss: 0.378610759973526\n",
      "Epoch 220, Loss: 0.375442236661911\n",
      "Epoch 230, Loss: 0.37220871448516846\n",
      "Epoch 240, Loss: 0.3689116835594177\n",
      "Epoch 250, Loss: 0.3656046986579895\n",
      "Epoch 260, Loss: 0.36223453283309937\n",
      "Epoch 270, Loss: 0.3588308095932007\n",
      "Epoch 280, Loss: 0.35541024804115295\n",
      "Epoch 290, Loss: 0.3519420921802521\n",
      "Epoch 300, Loss: 0.3484331965446472\n",
      "Epoch 310, Loss: 0.34490084648132324\n",
      "Epoch 320, Loss: 0.3413550555706024\n",
      "Epoch 330, Loss: 0.33778461813926697\n",
      "Epoch 340, Loss: 0.3341865837574005\n",
      "Epoch 350, Loss: 0.3305794596672058\n",
      "Epoch 360, Loss: 0.32693204283714294\n",
      "Epoch 370, Loss: 0.3232974112033844\n",
      "Epoch 380, Loss: 0.319640189409256\n",
      "Epoch 390, Loss: 0.3159691393375397\n",
      "Epoch 400, Loss: 0.3122914731502533\n",
      "Epoch 410, Loss: 0.30860257148742676\n",
      "Epoch 420, Loss: 0.3049190044403076\n",
      "Epoch 430, Loss: 0.3012336492538452\n",
      "Epoch 440, Loss: 0.2975435256958008\n",
      "Epoch 450, Loss: 0.2938530743122101\n",
      "Epoch 460, Loss: 0.29018187522888184\n",
      "Epoch 470, Loss: 0.28649771213531494\n",
      "Epoch 480, Loss: 0.2828208804130554\n",
      "Epoch 490, Loss: 0.2791425287723541\n",
      "Epoch 500, Loss: 0.27548593282699585\n",
      "Epoch 510, Loss: 0.27183714509010315\n",
      "Epoch 520, Loss: 0.26821836829185486\n",
      "Epoch 530, Loss: 0.2645854651927948\n",
      "Epoch 540, Loss: 0.2609638273715973\n",
      "Epoch 550, Loss: 0.25738054513931274\n",
      "Epoch 560, Loss: 0.25380730628967285\n",
      "Epoch 570, Loss: 0.2502431571483612\n",
      "Epoch 580, Loss: 0.24670708179473877\n",
      "Epoch 590, Loss: 0.24318502843379974\n",
      "Epoch 600, Loss: 0.239681676030159\n",
      "Epoch 610, Loss: 0.23620830476284027\n",
      "Epoch 620, Loss: 0.23275545239448547\n",
      "Epoch 630, Loss: 0.22931845486164093\n",
      "Epoch 640, Loss: 0.22593802213668823\n",
      "Epoch 650, Loss: 0.22253850102424622\n",
      "Epoch 660, Loss: 0.2191775143146515\n",
      "Epoch 670, Loss: 0.21587525308132172\n",
      "Epoch 680, Loss: 0.2125677466392517\n",
      "Epoch 690, Loss: 0.20930373668670654\n",
      "Epoch 700, Loss: 0.2060707062482834\n",
      "Epoch 710, Loss: 0.2028607279062271\n",
      "Epoch 720, Loss: 0.19968250393867493\n",
      "Epoch 730, Loss: 0.19654656946659088\n",
      "Epoch 740, Loss: 0.19343791902065277\n",
      "Epoch 750, Loss: 0.19034650921821594\n",
      "Epoch 760, Loss: 0.18730291724205017\n",
      "Epoch 770, Loss: 0.18430233001708984\n",
      "Epoch 780, Loss: 0.1813131421804428\n",
      "Epoch 790, Loss: 0.17836962640285492\n",
      "Epoch 800, Loss: 0.1754702925682068\n",
      "Epoch 810, Loss: 0.17259077727794647\n",
      "Epoch 820, Loss: 0.16975446045398712\n",
      "Epoch 830, Loss: 0.16695131361484528\n",
      "Epoch 840, Loss: 0.16419737040996552\n",
      "Epoch 850, Loss: 0.16145704686641693\n",
      "Epoch 860, Loss: 0.15876929461956024\n",
      "Epoch 870, Loss: 0.1561097800731659\n",
      "Epoch 880, Loss: 0.15349648892879486\n",
      "Epoch 890, Loss: 0.15091361105442047\n",
      "Epoch 900, Loss: 0.14837342500686646\n",
      "Epoch 910, Loss: 0.14585527777671814\n",
      "Epoch 920, Loss: 0.14339737594127655\n",
      "Epoch 930, Loss: 0.14096298813819885\n",
      "Epoch 940, Loss: 0.1385691910982132\n",
      "Epoch 950, Loss: 0.13621419668197632\n",
      "Epoch 960, Loss: 0.13390204310417175\n",
      "Epoch 970, Loss: 0.13161742687225342\n",
      "Epoch 980, Loss: 0.12936817109584808\n",
      "Epoch 990, Loss: 0.12716086208820343\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, g, features, labels, optimizer, num_epochs=1000):\n",
    "    for epoch in range(num_epochs):\n",
    "        logits = model(g, features)\n",
    "        pred = logits.squeeze(1)\n",
    "\n",
    "        # Select only the labels corresponding to the nodes in the graph\n",
    "        selected_labels = labels[:g.number_of_nodes()]\n",
    "\n",
    "        loss = nn.MSELoss()(pred, selected_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_model(model, G_dgl, features, train_labels_tensor, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11:\n",
    "Predict and report the results for the nodes in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on the test set: 0.1782936155796051\n",
      "Mean Absolute Error on the test set: 0.3832123887033412\n",
      "R-squared score on the test set: 0.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    selected_features = features[:num_nodes]  # Select only the features corresponding to the nodes in the graph\n",
    "    logits_test = model(G_dgl, selected_features)\n",
    "    pred_test = logits_test.squeeze(1).numpy()\n",
    "\n",
    "# Calculate metrics on the test set\n",
    "mse = nn.MSELoss()(torch.tensor(pred_test[:len(test_labels)]), torch.tensor(test_labels).float())\n",
    "mae = mean_absolute_error(test_labels, pred_test[:len(test_labels)])\n",
    "r2 = r2_score(test_labels, pred_test[:len(test_labels)])\n",
    "\n",
    "print(f\"Mean Squared Error on the test set: {mse}\")\n",
    "print(f\"Mean Absolute Error on the test set: {mae}\")\n",
    "print(f\"R-squared score on the test set: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
